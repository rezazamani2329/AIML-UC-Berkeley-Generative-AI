{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bee2dc0-0bf3-4f12-8daf-31f1e96591d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt- GANs and Transformer and diffusion models for market predictions in finance and portfolion management "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f90701-d48f-4dff-9aa2-26bc35045a6a",
   "metadata": {},
   "source": [
    "python code for Transformer in market prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f6e17d-9f26-4296-8ec0-d2af0dbc4621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention, Dropout, GlobalAveragePooling1D\n",
    "\n",
    "# Load and preprocess market data\n",
    "data = pd.read_csv('market_data.csv')\n",
    "prices = data['Close'].values.reshape(-1, 1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_prices = scaler.fit_transform(prices)\n",
    "\n",
    "# Prepare training data\n",
    "seq_length = 60\n",
    "X_train, y_train = [], []\n",
    "\n",
    "for i in range(len(scaled_prices) - seq_length):\n",
    "    X_train.append(scaled_prices[i:i + seq_length])\n",
    "    y_train.append(scaled_prices[i + seq_length])\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "# Build the Transformer model\n",
    "def build_transformer_model(seq_length, feature_dim, num_heads=4, ff_dim=64, num_layers=2):\n",
    "    inputs = Input(shape=(seq_length, feature_dim))\n",
    "    x = inputs\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=feature_dim)(x, x)\n",
    "        attn_output = Dropout(0.1)(attn_output)\n",
    "        x = LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "\n",
    "        ff_output = Dense(ff_dim, activation='relu')(x)\n",
    "        ff_output = Dense(feature_dim)(ff_output)\n",
    "        ff_output = Dropout(0.1)(ff_output)\n",
    "        x = LayerNormalization(epsilon=1e-6)(x + ff_output)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    outputs = Dense(feature_dim)(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_transformer_model(seq_length, 1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_train[-1].reshape(1, seq_length, 1))\n",
    "predicted_price = scaler.inverse_transform(predictions)\n",
    "\n",
    "print(predicted_price)\n",
    "\n",
    "# Save the model\n",
    "model.save('market_prediction_transformer.h5')\n",
    "\n",
    "# This code builds a Transformer to predict future market prices based on historical data.\n",
    "# Let me know if youâ€™d like me to fine-tune the architecture, add features, or improve the training process!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf21bd3-3a5c-4988-95a5-f32db52267ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9569d2f1-b8c7-4b1c-ad69-1a12df10a9c6",
   "metadata": {},
   "source": [
    "python code for Transformer in portfolio management\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948bcabc-60d5-4470-8cb1-546f241fded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention, Dropout, GlobalAveragePooling1D\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_csv('portfolio_data.csv')\n",
    "returns = data.pct_change().dropna()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_returns = scaler.fit_transform(returns)\n",
    "\n",
    "# Prepare training data\n",
    "seq_length = 60\n",
    "X_train, y_train = [], []\n",
    "\n",
    "for i in range(len(scaled_returns) - seq_length):\n",
    "    X_train.append(scaled_returns[i:i + seq_length])\n",
    "    y_train.append(scaled_returns[i + seq_length])\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "# Build the Transformer model\n",
    "def build_transformer_model(seq_length, feature_dim, num_heads=4, ff_dim=64, num_layers=2):\n",
    "    inputs = Input(shape=(seq_length, feature_dim))\n",
    "    x = inputs\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=feature_dim)(x, x)\n",
    "        attn_output = Dropout(0.1)(attn_output)\n",
    "        x = LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "\n",
    "        ff_output = Dense(ff_dim, activation='relu')(x)\n",
    "        ff_output = Dense(feature_dim)(ff_output)\n",
    "        ff_output = Dropout(0.1)(ff_output)\n",
    "        x = LayerNormalization(epsilon=1e-6)(x + ff_output)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    outputs = Dense(feature_dim)(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_transformer_model(seq_length, returns.shape[1])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_train[-1].reshape(1, seq_length, returns.shape[1]))\n",
    "predicted_returns = scaler.inverse_transform(predictions)\n",
    "\n",
    "print(predicted_returns)\n",
    "\n",
    "# Save the model\n",
    "model.save('portfolio_management_transformer.h5')\n",
    "\n",
    "# This code sets up a Transformer to analyze historical portfolio returns and predict future returns.\n",
    "# Let me know if youâ€™d like me to adjust the architecture, add evaluation metrics, or enhance the training process!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3181e841-8c7d-49a7-a17c-83688b06787b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a534ed02-9263-437b-b26e-88e6ead760ca",
   "metadata": {},
   "source": [
    "python code for GANs in portfolio management\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9a3e43-aa0f-48c3-8e93-6409afce0c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, Reshape, LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_csv('portfolio_data.csv')\n",
    "returns = data.pct_change().dropna()\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_returns = scaler.fit_transform(returns)\n",
    "\n",
    "# Prepare training data\n",
    "seq_length = 60\n",
    "X_train = []\n",
    "\n",
    "for i in range(len(scaled_returns) - seq_length):\n",
    "    X_train.append(scaled_returns[i:i + seq_length])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "# Build the GAN\n",
    "def build_generator():\n",
    "    model = Sequential([\n",
    "        Dense(128, input_dim=100),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(256),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(seq_length * returns.shape[1]),\n",
    "        Reshape((seq_length, returns.shape[1]))\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_discriminator():\n",
    "    model = Sequential([\n",
    "        LSTM(64, return_sequences=True, input_shape=(seq_length, returns.shape[1])),\n",
    "        LSTM(32),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "discriminator.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "discriminator.trainable = False\n",
    "\n",
    "gan_input = tf.keras.Input(shape=(100,))\n",
    "generated_returns = generator(gan_input)\n",
    "validity = discriminator(generated_returns)\n",
    "\n",
    "gan = tf.keras.Model(gan_input, validity)\n",
    "gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Train the GAN\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "    generated_sequences = generator.predict(noise)\n",
    "    \n",
    "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "    real_sequences = X_train[idx]\n",
    "    \n",
    "    d_loss_real = discriminator.train_on_batch(real_sequences, np.ones((batch_size, 1)))\n",
    "    d_loss_fake = discriminator.train_on_batch(generated_sequences, np.zeros((batch_size, 1)))\n",
    "    d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
    "    \n",
    "    g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, D Loss: {d_loss}, G Loss: {g_loss}\")\n",
    "\n",
    "# Generate synthetic portfolio scenarios\n",
    "noise = np.random.normal(0, 1, (1, 100))\n",
    "generated_returns = generator.predict(noise)\n",
    "simulated_returns = scaler.inverse_transform(generated_returns.reshape(seq_length, returns.shape[1]))\n",
    "\n",
    "print(simulated_returns)\n",
    "\n",
    "# Save the model\n",
    "generator.save('portfolio_management_gan.h5')\n",
    "\n",
    "# This script sets up a GAN to simulate portfolio return scenarios, aiding in risk assessment and strategy testing.\n",
    "# Let me know if youâ€™d like me to refine or expand it further!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c35401-d95e-4d57-a0dd-84667956ed97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c943f3ed-1347-457e-9e08-241f5bbbb6ca",
   "metadata": {},
   "source": [
    "python code for GANs in market prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e97289-dbba-4daa-b388-f53d82afd2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Reshape, LeakyReLU\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_csv('market_data.csv')\n",
    "prices = data['Close'].values.reshape(-1, 1)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "prices_scaled = scaler.fit_transform(prices)\n",
    "\n",
    "# Prepare training data\n",
    "seq_length = 60\n",
    "X_train, y_train = [], []\n",
    "for i in range(len(prices_scaled) - seq_length):\n",
    "    X_train.append(prices_scaled[i:i + seq_length])\n",
    "    y_train.append(prices_scaled[i + seq_length])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "# Build the GAN\n",
    "def build_generator():\n",
    "    model = Sequential([\n",
    "        Dense(128, input_dim=100),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(256),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(seq_length),\n",
    "        Reshape((seq_length, 1))\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_discriminator():\n",
    "    model = Sequential([\n",
    "        LSTM(64, return_sequences=True, input_shape=(seq_length, 1)),\n",
    "        LSTM(32),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "discriminator.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "discriminator.trainable = False\n",
    "\n",
    "gan_input = tf.keras.Input(shape=(100,))\n",
    "generated_sequence = generator(gan_input)\n",
    "validity = discriminator(generated_sequence)\n",
    "\n",
    "gan = tf.keras.Model(gan_input, validity)\n",
    "gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Train the GAN\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "    generated_sequences = generator.predict(noise)\n",
    "    \n",
    "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "    real_sequences = X_train[idx]\n",
    "    \n",
    "    d_loss_real = discriminator.train_on_batch(real_sequences, np.ones((batch_size, 1)))\n",
    "    d_loss_fake = discriminator.train_on_batch(generated_sequences, np.zeros((batch_size, 1)))\n",
    "    d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
    "    \n",
    "    g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, D Loss: {d_loss}, G Loss: {g_loss}\")\n",
    "\n",
    "# Generate market predictions\n",
    "noise = np.random.normal(0, 1, (1, 100))\n",
    "generated_sequence = generator.predict(noise)\n",
    "predicted_prices = scaler.inverse_transform(generated_sequence.reshape(-1, 1))\n",
    "\n",
    "print(predicted_prices)\n",
    "\n",
    "# Save the model\n",
    "generator.save('market_predictor_gan.h5')\n",
    "\n",
    "# This script sets up a basic GAN to predict market prices based on historical data sequences.\n",
    "# Let me know if youâ€™d like me to refine or expand it further!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12296250-8126-48fc-bdad-4482aea4b204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9408d4-d989-4e4b-a124-5d9b34f53229",
   "metadata": {},
   "outputs": [],
   "source": [
    "market prediction with diffusion model in GenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edba012d-3772-4daf-8bc0-c3b69ed175bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "# Bass diffusion model for GenAI adoption\n",
    "def bass_model(N, t, p, q, M):\n",
    "    dNdt = (p + q * (N / M)) * (M - N)\n",
    "    return dNdt\n",
    "\n",
    "# Parameters for GenAI adoption\n",
    "M = 10000  # Market potential (total potential users)\n",
    "p = 0.05   # Coefficient of innovation (early adopters)\n",
    "q = 0.4    # Coefficient of imitation (network effects)\n",
    "\n",
    "# Time range\n",
    "t = np.linspace(0, 50, 500)\n",
    "\n",
    "# Initial condition (initial adopters)\n",
    "N0 = 10\n",
    "\n",
    "# Solve the differential equation\n",
    "N = odeint(bass_model, N0, t, args=(p, q, M))\n",
    "\n",
    "# Plotting the adoption curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(t, N, label='GenAI Adopters over Time', color='purple')\n",
    "plt.xlabel('Time (years)')\n",
    "plt.ylabel('Number of Adopters')\n",
    "plt.title('Market Prediction for Generative AI Adoption (Bass Model)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Let me know if youâ€™d like to add more complexity, like external shocks or real-world data integration! ðŸš€\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
