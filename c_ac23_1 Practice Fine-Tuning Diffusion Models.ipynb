{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kPumybzhfzDz"
      },
      "source": [
        "### Self-Study Colab Activity 23.1: Practice Fine-Tuning Diffusion Models  \n",
        "\n",
        "**Expected Time = 90 minutes**\n",
        "\n",
        "\n",
        "In this activity you will practice fine-tuning a diffusion model provided by [ai-toolkit](https://github.com/ostris/ai-toolkit/tree/main) to personalize AI images created by the model using a dataset of your choice.\n",
        "\n",
        "Before you attempt this activity, ensure you have setup Hugging Face and that you have a personal Read Token available to you.\n",
        "\n",
        "Additionally, in order the complete the training, ensure you are using Google Colab Pro.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLRADhDfh2YV"
      },
      "source": [
        "#### Part 1: Choose the Correct Runtime in Google Colab.\n",
        "\n",
        "You will need to use the GPU runtime available in Google Colab to ensure that you can complete the training of the AI model.\n",
        "\n",
        "To change the runtime follow the steps below:\n",
        "\n",
        "1) Click on `Runtime` menu at the top and select `Change runtime type`.\n",
        "\n",
        "2) Select the `A100 GPU` and click on `Save`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AOGiY4Li4Bd"
      },
      "source": [
        "#### Part 2: Create a Database for Training\n",
        "\n",
        "The AI Toolkit requires you to create a personalized database of images that the diffusion model will use to fine-tune the AI images available in the library.\n",
        "\n",
        "To create a paersonalized database, follow the steps below:\n",
        "\n",
        "1) Click on the `Files` icon in the menu on the left.\n",
        "\n",
        "2) Double click on `Files` menu and create a new folder and name it `dataset`.\n",
        "\n",
        "3) Upload ten images of yourself inside the `dataset` folder. Ensure all images are in  `.jpg`, `.jpeg`, or `.png` format and they are saved as `img1.jpg`, `img2.jpg`, ..., `img10.jpg`.\n",
        "\n",
        "4) To each picture should correspond a `.txt` file containing a short description of what the image represent for example `An image of your_name`. Ensure all text files are saved as `img1.txt`, `img2.txt`, ..., `img10.txt`. It is imperative that images and the text files follow exactly this naming convention.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWSsAdiJmRyv"
      },
      "source": [
        "#### Part 3: Install the Necessary Libraries\n",
        "\n",
        "Run the code cell below to install the necessary libraries. Note that you might get a warning that you need to restart the runtime, but this is not necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WMZtoyrZnny1",
        "outputId": "3f3a0fd8-19e5-4026-959c-bf85aaccc59a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'ai-toolkit'...\n",
            "remote: Enumerating objects: 3721, done.\u001b[K\n",
            "remote: Counting objects: 100% (1208/1208), done.\u001b[K\n",
            "remote: Compressing objects: 100% (150/150), done.\u001b[K\n",
            "remote: Total 3721 (delta 1136), reused 1093 (delta 1058), pack-reused 2513 (from 1)\u001b[K\n",
            "Receiving objects: 100% (3721/3721), 29.72 MiB | 24.40 MiB/s, done.\n",
            "Resolving deltas: 100% (2826/2826), done.\n",
            "Submodule 'repositories/batch_annotator' (https://github.com/ostris/batch-annotator) registered for path 'repositories/batch_annotator'\n",
            "Submodule 'repositories/ipadapter' (https://github.com/tencent-ailab/IP-Adapter.git) registered for path 'repositories/ipadapter'\n",
            "Submodule 'repositories/leco' (https://github.com/p1atdev/LECO) registered for path 'repositories/leco'\n",
            "Submodule 'repositories/sd-scripts' (https://github.com/kohya-ss/sd-scripts.git) registered for path 'repositories/sd-scripts'\n",
            "Cloning into '/content/ai-toolkit/repositories/batch_annotator'...\n",
            "Cloning into '/content/ai-toolkit/repositories/ipadapter'...\n",
            "Cloning into '/content/ai-toolkit/repositories/leco'...\n",
            "Cloning into '/content/ai-toolkit/repositories/sd-scripts'...\n",
            "Submodule path 'repositories/batch_annotator': checked out '420e142f6ad3cc14b3ea0500affc2c6c7e7544bf'\n",
            "Submodule 'repositories/controlnet' (https://github.com/lllyasviel/ControlNet-v1-1-nightly.git) registered for path 'repositories/batch_annotator/repositories/controlnet'\n",
            "Cloning into '/content/ai-toolkit/repositories/batch_annotator/repositories/controlnet'...\n",
            "Submodule path 'repositories/batch_annotator/repositories/controlnet': checked out 'e2b44154b72965c5e11b1ccee941d550682e4701'\n",
            "Submodule path 'repositories/ipadapter': checked out '5a18b1f3660acaf8bee8250692d6fb3548a19b14'\n",
            "Submodule path 'repositories/leco': checked out '9294adf40218e917df4516737afb13f069a6789d'\n",
            "Submodule path 'repositories/sd-scripts': checked out 'b78c0e2a69e52ce6c79abc6c8c82d1a9cabcf05c'\n",
            "Collecting git+https://github.com/huggingface/diffusers.git (from -r ai-toolkit/requirements.txt (line 4))\n",
            "  Cloning https://github.com/huggingface/diffusers.git to /tmp/pip-req-build-rx6_3u7e\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/diffusers.git /tmp/pip-req-build-rx6_3u7e\n",
            "  Resolved https://github.com/huggingface/diffusers.git to commit 48e36353d8cbf0322ec1ad0684b95d11f70af2de\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r ai-toolkit/requirements.txt (line 1)) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r ai-toolkit/requirements.txt (line 2)) (0.19.0+cu121)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from -r ai-toolkit/requirements.txt (line 3)) (0.4.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r ai-toolkit/requirements.txt (line 5)) (4.44.2)\n",
            "Collecting lycoris-lora==1.8.3 (from -r ai-toolkit/requirements.txt (line 6))\n",
            "  Downloading lycoris_lora-1.8.3.tar.gz (96 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting flatten_json (from -r ai-toolkit/requirements.txt (line 7))\n",
            "  Downloading flatten_json-0.1.14-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from -r ai-toolkit/requirements.txt (line 8)) (6.0.2)\n",
            "Collecting oyaml (from -r ai-toolkit/requirements.txt (line 9))\n",
            "  Downloading oyaml-1.0-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r ai-toolkit/requirements.txt (line 10)) (2.17.0)\n",
            "Collecting kornia (from -r ai-toolkit/requirements.txt (line 11))\n",
            "  Downloading kornia-0.7.3-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting invisible-watermark (from -r ai-toolkit/requirements.txt (line 12))\n",
            "  Downloading invisible_watermark-0.2.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from -r ai-toolkit/requirements.txt (line 13)) (0.8.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from -r ai-toolkit/requirements.txt (line 14)) (0.34.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from -r ai-toolkit/requirements.txt (line 15)) (0.10.2)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (from -r ai-toolkit/requirements.txt (line 16)) (1.4.14)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from -r ai-toolkit/requirements.txt (line 17)) (2.9.1)\n",
            "Collecting omegaconf (from -r ai-toolkit/requirements.txt (line 18))\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting k-diffusion (from -r ai-toolkit/requirements.txt (line 19))\n",
            "  Downloading k_diffusion-0.1.1.post1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting open_clip_torch (from -r ai-toolkit/requirements.txt (line 20))\n",
            "  Downloading open_clip_torch-2.26.1-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting timm (from -r ai-toolkit/requirements.txt (line 21))\n",
            "  Downloading timm-1.0.9-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting prodigyopt (from -r ai-toolkit/requirements.txt (line 22))\n",
            "  Downloading prodigyopt-1.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting controlnet_aux==0.0.7 (from -r ai-toolkit/requirements.txt (line 23))\n",
            "  Downloading controlnet_aux-0.0.7.tar.gz (202 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.4/202.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting python-dotenv (from -r ai-toolkit/requirements.txt (line 24))\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting bitsandbytes (from -r ai-toolkit/requirements.txt (line 25))\n",
            "  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting hf_transfer (from -r ai-toolkit/requirements.txt (line 26))\n",
            "  Downloading hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting lpips (from -r ai-toolkit/requirements.txt (line 27))\n",
            "  Downloading lpips-0.1.4-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pytorch_fid (from -r ai-toolkit/requirements.txt (line 28))\n",
            "  Downloading pytorch_fid-0.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting optimum-quanto (from -r ai-toolkit/requirements.txt (line 29))\n",
            "  Downloading optimum_quanto-0.2.4-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from -r ai-toolkit/requirements.txt (line 30)) (0.1.99)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from -r ai-toolkit/requirements.txt (line 31)) (0.24.6)\n",
            "Collecting peft (from -r ai-toolkit/requirements.txt (line 32))\n",
            "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting gradio (from -r ai-toolkit/requirements.txt (line 33))\n",
            "  Downloading gradio-4.44.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from -r ai-toolkit/requirements.txt (line 34)) (8.0.4)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.10/dist-packages (from controlnet_aux==0.0.7->-r ai-toolkit/requirements.txt (line 23)) (8.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from controlnet_aux==0.0.7->-r ai-toolkit/requirements.txt (line 23)) (1.13.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from controlnet_aux==0.0.7->-r ai-toolkit/requirements.txt (line 23)) (4.10.0.84)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from controlnet_aux==0.0.7->-r ai-toolkit/requirements.txt (line 23)) (3.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from controlnet_aux==0.0.7->-r ai-toolkit/requirements.txt (line 23)) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from controlnet_aux==0.0.7->-r ai-toolkit/requirements.txt (line 23)) (9.4.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from controlnet_aux==0.0.7->-r ai-toolkit/requirements.txt (line 23)) (0.23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r ai-toolkit/requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r ai-toolkit/requirements.txt (line 1)) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r ai-toolkit/requirements.txt (line 1)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r ai-toolkit/requirements.txt (line 1)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r ai-toolkit/requirements.txt (line 1)) (2024.6.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.31.0.dev0->-r ai-toolkit/requirements.txt (line 4)) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers==0.31.0.dev0->-r ai-toolkit/requirements.txt (line 4)) (2.32.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->-r ai-toolkit/requirements.txt (line 5)) (24.1)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->-r ai-toolkit/requirements.txt (line 5)) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->-r ai-toolkit/requirements.txt (line 5)) (4.66.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from flatten_json->-r ai-toolkit/requirements.txt (line 7)) (1.16.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r ai-toolkit/requirements.txt (line 10)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r ai-toolkit/requirements.txt (line 10)) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r ai-toolkit/requirements.txt (line 10)) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r ai-toolkit/requirements.txt (line 10)) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r ai-toolkit/requirements.txt (line 10)) (71.0.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r ai-toolkit/requirements.txt (line 10)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r ai-toolkit/requirements.txt (line 10)) (3.0.4)\n",
            "Collecting kornia-rs>=0.1.0 (from kornia->-r ai-toolkit/requirements.txt (line 11))\n",
            "  Downloading kornia_rs-0.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n",
            "Collecting PyWavelets>=1.1.1 (from invisible-watermark->-r ai-toolkit/requirements.txt (line 12))\n",
            "  Downloading pywavelets-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r ai-toolkit/requirements.txt (line 14)) (5.9.5)\n",
            "Requirement already satisfied: albucore>=0.0.13 in /usr/local/lib/python3.10/dist-packages (from albumentations->-r ai-toolkit/requirements.txt (line 16)) (0.0.14)\n",
            "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations->-r ai-toolkit/requirements.txt (line 16)) (0.2.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations->-r ai-toolkit/requirements.txt (line 16)) (4.10.0.84)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->-r ai-toolkit/requirements.txt (line 17)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic->-r ai-toolkit/requirements.txt (line 17)) (2.23.3)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->-r ai-toolkit/requirements.txt (line 18))\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting clean-fid (from k-diffusion->-r ai-toolkit/requirements.txt (line 19))\n",
            "  Downloading clean_fid-0.1.35-py3-none-any.whl.metadata (36 kB)\n",
            "Collecting clip-anytorch (from k-diffusion->-r ai-toolkit/requirements.txt (line 19))\n",
            "  Downloading clip_anytorch-2.6.0-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting dctorch (from k-diffusion->-r ai-toolkit/requirements.txt (line 19))\n",
            "  Downloading dctorch-0.1.2-py3-none-any.whl.metadata (607 bytes)\n",
            "Collecting jsonmerge (from k-diffusion->-r ai-toolkit/requirements.txt (line 19))\n",
            "  Downloading jsonmerge-1.9.2-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting torchdiffeq (from k-diffusion->-r ai-toolkit/requirements.txt (line 19))\n",
            "  Downloading torchdiffeq-0.2.4-py3-none-any.whl.metadata (440 bytes)\n",
            "Collecting torchsde (from k-diffusion->-r ai-toolkit/requirements.txt (line 19))\n",
            "  Downloading torchsde-0.2.6-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting wandb (from k-diffusion->-r ai-toolkit/requirements.txt (line 19))\n",
            "  Downloading wandb-0.18.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting ftfy (from open_clip_torch->-r ai-toolkit/requirements.txt (line 20))\n",
            "  Downloading ftfy-6.2.3-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting ninja (from optimum-quanto->-r ai-toolkit/requirements.txt (line 29))\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio->-r ai-toolkit/requirements.txt (line 33))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r ai-toolkit/requirements.txt (line 33)) (3.7.1)\n",
            "Collecting fastapi<1.0 (from gradio->-r ai-toolkit/requirements.txt (line 33))\n",
            "  Downloading fastapi-0.114.1-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio->-r ai-toolkit/requirements.txt (line 33))\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.3.0 (from gradio->-r ai-toolkit/requirements.txt (line 33))\n",
            "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio->-r ai-toolkit/requirements.txt (line 33))\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio->-r ai-toolkit/requirements.txt (line 33)) (6.4.5)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r ai-toolkit/requirements.txt (line 33)) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r ai-toolkit/requirements.txt (line 33)) (3.7.1)\n",
            "Collecting orjson~=3.0 (from gradio->-r ai-toolkit/requirements.txt (line 33))\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r ai-toolkit/requirements.txt (line 33)) (2.1.4)\n",
            "Collecting pydub (from gradio->-r ai-toolkit/requirements.txt (line 33))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio->-r ai-toolkit/requirements.txt (line 33))\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting ruff>=0.2.2 (from gradio->-r ai-toolkit/requirements.txt (line 33))\n",
            "  Downloading ruff-0.6.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio->-r ai-toolkit/requirements.txt (line 33))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio->-r ai-toolkit/requirements.txt (line 33))\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio->-r ai-toolkit/requirements.txt (line 33)) (0.12.5)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r ai-toolkit/requirements.txt (line 33)) (2.0.7)\n",
            "Collecting uvicorn>=0.14.0 (from gradio->-r ai-toolkit/requirements.txt (line 33))\n",
            "  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio->-r ai-toolkit/requirements.txt (line 33))\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->-r ai-toolkit/requirements.txt (line 34)) (1.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio->-r ai-toolkit/requirements.txt (line 33)) (3.8)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio->-r ai-toolkit/requirements.txt (line 33)) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio->-r ai-toolkit/requirements.txt (line 33)) (1.2.2)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi<1.0->gradio->-r ai-toolkit/requirements.txt (line 33))\n",
            "  Downloading starlette-0.38.5-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio->-r ai-toolkit/requirements.txt (line 33)) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio->-r ai-toolkit/requirements.txt (line 33))\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio->-r ai-toolkit/requirements.txt (line 33))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r ai-toolkit/requirements.txt (line 33)) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r ai-toolkit/requirements.txt (line 33)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r ai-toolkit/requirements.txt (line 33)) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r ai-toolkit/requirements.txt (line 33)) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r ai-toolkit/requirements.txt (line 33)) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r ai-toolkit/requirements.txt (line 33)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio->-r ai-toolkit/requirements.txt (line 33)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio->-r ai-toolkit/requirements.txt (line 33)) (2024.1)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->controlnet_aux==0.0.7->-r ai-toolkit/requirements.txt (line 23)) (2.34.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->controlnet_aux==0.0.7->-r ai-toolkit/requirements.txt (line 23)) (2024.8.30)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->controlnet_aux==0.0.7->-r ai-toolkit/requirements.txt (line 23)) (0.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio->-r ai-toolkit/requirements.txt (line 33)) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio->-r ai-toolkit/requirements.txt (line 33)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio->-r ai-toolkit/requirements.txt (line 33)) (13.8.1)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->open_clip_torch->-r ai-toolkit/requirements.txt (line 20)) (0.2.13)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata->controlnet_aux==0.0.7->-r ai-toolkit/requirements.txt (line 23)) (3.20.1)\n",
            "Requirement already satisfied: jsonschema>2.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonmerge->k-diffusion->-r ai-toolkit/requirements.txt (line 19)) (4.23.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.31.0.dev0->-r ai-toolkit/requirements.txt (line 4)) (3.3.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r ai-toolkit/requirements.txt (line 1)) (1.3.0)\n",
            "Collecting trampoline>=0.1.2 (from torchsde->k-diffusion->-r ai-toolkit/requirements.txt (line 19))\n",
            "  Downloading trampoline-0.1.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb->k-diffusion->-r ai-toolkit/requirements.txt (line 19))\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb->k-diffusion->-r ai-toolkit/requirements.txt (line 19))\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb->k-diffusion->-r ai-toolkit/requirements.txt (line 19)) (4.3.2)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb->k-diffusion->-r ai-toolkit/requirements.txt (line 19))\n",
            "  Downloading sentry_sdk-2.14.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting setproctitle (from wandb->k-diffusion->-r ai-toolkit/requirements.txt (line 19))\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb->k-diffusion->-r ai-toolkit/requirements.txt (line 19))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>2.4.0->jsonmerge->k-diffusion->-r ai-toolkit/requirements.txt (line 19)) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>2.4.0->jsonmerge->k-diffusion->-r ai-toolkit/requirements.txt (line 19)) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>2.4.0->jsonmerge->k-diffusion->-r ai-toolkit/requirements.txt (line 19)) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>2.4.0->jsonmerge->k-diffusion->-r ai-toolkit/requirements.txt (line 19)) (0.20.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio->-r ai-toolkit/requirements.txt (line 33)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio->-r ai-toolkit/requirements.txt (line 33)) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->k-diffusion->-r ai-toolkit/requirements.txt (line 19))\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio->-r ai-toolkit/requirements.txt (line 33)) (0.1.2)\n",
            "Downloading flatten_json-0.1.14-py3-none-any.whl (8.0 kB)\n",
            "Downloading oyaml-1.0-py2.py3-none-any.whl (3.0 kB)\n",
            "Downloading kornia-0.7.3-py2.py3-none-any.whl (833 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m833.3/833.3 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading invisible_watermark-0.2.0-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading k_diffusion-0.1.1.post1-py3-none-any.whl (33 kB)\n",
            "Downloading open_clip_torch-2.26.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading timm-1.0.9-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prodigyopt-1.0-py3-none-any.whl (5.5 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lpips-0.1.4-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_fid-0.3.0-py3-none-any.whl (15 kB)\n",
            "Downloading optimum_quanto-0.2.4-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.7/109.7 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-4.44.0-py3-none-any.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kornia_rs-0.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Downloading pywavelets-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruff-0.6.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m124.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading clean_fid-0.1.35-py3-none-any.whl (26 kB)\n",
            "Downloading clip_anytorch-2.6.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dctorch-0.1.2-py3-none-any.whl (2.3 kB)\n",
            "Downloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading ftfy-6.2.3-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.0/43.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonmerge-1.9.2-py3-none-any.whl (19 kB)\n",
            "Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading torchdiffeq-0.2.4-py3-none-any.whl (32 kB)\n",
            "Downloading torchsde-0.2.6-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wandb-0.18.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m122.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.14.0-py2.py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.4/311.4 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.38.5-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trampoline-0.1.2-py3-none-any.whl (5.2 kB)\n",
            "Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: lycoris-lora, controlnet_aux, diffusers, antlr4-python3-runtime\n",
            "  Building wheel for lycoris-lora (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lycoris-lora: filename=lycoris_lora-1.8.3-py3-none-any.whl size=77134 sha256=3881280c72c3779f02fcda60ea5872351f24d850bcb25dfdec13205fb4061a29\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/d8/ac/e1feba5dec18685dac32ff2465ea1908cbe6a919a0c008a215\n",
            "  Building wheel for controlnet_aux (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for controlnet_aux: filename=controlnet_aux-0.0.7-py3-none-any.whl size=274342 sha256=faaa5ee22a4853e21bc7cea5fbc42a50517dbb3c5f7670e0e59aeba311f65c6a\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/3e/93/6678b4c0bc2ec31d53409b25d4189cbb08bae843e8b2b78e52\n",
            "  Building wheel for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for diffusers: filename=diffusers-0.31.0.dev0-py3-none-any.whl size=2739327 sha256=dc1b883bdf8e7ad9d27f9afc470106dcd9955188262d93602033bc0d02813ee0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pndfnqsh/wheels/4d/b7/a8/6f9549ceec5daad78675b857ac57d697c387062506520a7b50\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=30492d01c94cc89c47b3472fdb07bfd5091b6927e7a163ada556a37e8417fe00\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built lycoris-lora controlnet_aux diffusers antlr4-python3-runtime\n",
            "Installing collected packages: trampoline, pydub, ninja, antlr4-python3-runtime, websockets, tomlkit, smmap, setproctitle, sentry-sdk, semantic-version, ruff, PyWavelets, python-multipart, python-dotenv, prodigyopt, oyaml, orjson, omegaconf, kornia-rs, hf_transfer, h11, ftfy, flatten_json, ffmpy, docker-pycreds, aiofiles, uvicorn, starlette, httpcore, gitdb, torchsde, torchdiffeq, optimum-quanto, kornia, invisible-watermark, httpx, gitpython, fastapi, diffusers, dctorch, bitsandbytes, wandb, timm, pytorch_fid, lpips, jsonmerge, gradio-client, clip-anytorch, clean-fid, peft, open_clip_torch, lycoris-lora, k-diffusion, gradio, controlnet_aux\n",
            "  Attempting uninstall: tomlkit\n",
            "    Found existing installation: tomlkit 0.13.2\n",
            "    Uninstalling tomlkit-0.13.2:\n",
            "      Successfully uninstalled tomlkit-0.13.2\n",
            "Successfully installed PyWavelets-1.7.0 aiofiles-23.2.1 antlr4-python3-runtime-4.9.3 bitsandbytes-0.43.3 clean-fid-0.1.35 clip-anytorch-2.6.0 controlnet_aux-0.0.7 dctorch-0.1.2 diffusers-0.31.0.dev0 docker-pycreds-0.4.0 fastapi-0.114.1 ffmpy-0.4.0 flatten_json-0.1.14 ftfy-6.2.3 gitdb-4.0.11 gitpython-3.1.43 gradio-4.44.0 gradio-client-1.3.0 h11-0.14.0 hf_transfer-0.1.8 httpcore-1.0.5 httpx-0.27.2 invisible-watermark-0.2.0 jsonmerge-1.9.2 k-diffusion-0.1.1.post1 kornia-0.7.3 kornia-rs-0.1.5 lpips-0.1.4 lycoris-lora-1.8.3 ninja-1.11.1.1 omegaconf-2.3.0 open_clip_torch-2.26.1 optimum-quanto-0.2.4 orjson-3.10.7 oyaml-1.0 peft-0.12.0 prodigyopt-1.0 pydub-0.25.1 python-dotenv-1.0.1 python-multipart-0.0.9 pytorch_fid-0.3.0 ruff-0.6.5 semantic-version-2.10.0 sentry-sdk-2.14.0 setproctitle-1.3.3 smmap-5.0.1 starlette-0.38.5 timm-1.0.9 tomlkit-0.12.0 torchdiffeq-0.2.4 torchsde-0.2.6 trampoline-0.1.2 uvicorn-0.30.6 wandb-0.18.0 websockets-12.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "243a1dbf6a2e4537ad8e17132cb60764",
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!git clone https://github.com/ostris/ai-toolkit.git\n",
        "!cd ai-toolkit && git submodule update --init --recursive\n",
        "!pip3 install -r ai-toolkit/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6LP8tv9orgT"
      },
      "source": [
        "To ensure that the libraries were installed correctly, verify that the `ai-toolkit` folder is in the file tree on the left."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "M38TFPXepR3U"
      },
      "source": [
        "#### Part 4: Connect to Hugging Face\n",
        "\n",
        "To ensure that you can run the `FLUX.1-dev` model, you will need to connect to Hugging Face using the Read token you setup earlier.\n",
        "\n",
        "To connect to Hugging Face follow the steps below:\n",
        "\n",
        "1) Navigate inside the `ai-toolkit` folder. At the root of this folder, create a new file. Leave the default name `untitled` for now.\n",
        "\n",
        "2) Double-click on the `untitled` file and copy the following inside it: `HF_TOKEN=your_key_here`. Be sure to replace `your_key_here` with your Read Token from Hugging Face.\n",
        "\n",
        "3) Save the changes to the `untitled` file.\n",
        "\n",
        "4) In the filetree on the left, right-click on the  `untitled` file and rename it to `.env`. Note that this file will become hidden and your won't be able to see it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTjplQShsI0H"
      },
      "source": [
        "#### Part 5: Edit the Configuration File\n",
        "\n",
        "The last step before you train the model involves adjusting the configuration file to ensure that your model trains correclty.\n",
        "\n",
        "To configure your model, follow the steps below:\n",
        "\n",
        "1) From the `ai-toolkit` folder navigate inside the `config` folder and then inside the `examples` folder.\n",
        "\n",
        "2) Double-click on the `train_lora_flux_24gb.yaml` file to open it. This files contains a number of configuration options to train your model as you desire.\n",
        "\n",
        "3) Make the following modifications to the `train_lora_flux_24gb.yaml` file:\n",
        "\n",
        "- Change the value of `folder_path` to `\"/content/dataset\"`.\n",
        "\n",
        "Save the changes to the `train_lora_flux_24gb.yaml` file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqjQ9SyOvSKf"
      },
      "source": [
        "#### Part 6: Train the Model\n",
        "\n",
        "Run the code cell below to run the model.\n",
        "\n",
        "Note that the training should take about 1 hour to complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yh4L8zmOvCSK",
        "outputId": "fc5362a0-06ba-431d-d1fc-3e65b7c862ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running 1 job\n",
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
            "0it [00:00, ?it/s]\n",
            "2024-09-13 18:55:29.181091: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-09-13 18:55:29.199341: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-13 18:55:29.220772: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-13 18:55:29.227319: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-13 18:55:29.243127: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-09-13 18:55:30.424248: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.15 (you have 1.4.14). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "/usr/local/lib/python3.10/dist-packages/controlnet_aux/mediapipe_face/mediapipe_face_common.py:7: UserWarning: The module 'mediapipe' is not installed. The package will have limited functionality. Please install it using the command: pip install 'mediapipe'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_5m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  return register_model(fn_wrapper)\n",
            "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_11m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  return register_model(fn_wrapper)\n",
            "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  return register_model(fn_wrapper)\n",
            "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_384 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  return register_model(fn_wrapper)\n",
            "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_512 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  return register_model(fn_wrapper)\n",
            "{\n",
            "    \"type\": \"sd_trainer\",\n",
            "    \"training_folder\": \"output\",\n",
            "    \"device\": \"cuda:0\",\n",
            "    \"network\": {\n",
            "        \"type\": \"lora\",\n",
            "        \"linear\": 16,\n",
            "        \"linear_alpha\": 16\n",
            "    },\n",
            "    \"save\": {\n",
            "        \"dtype\": \"float16\",\n",
            "        \"save_every\": 250,\n",
            "        \"max_step_saves_to_keep\": 4,\n",
            "        \"push_to_hub\": false\n",
            "    },\n",
            "    \"datasets\": [\n",
            "        {\n",
            "            \"folder_path\": \"/content/dataset\",\n",
            "            \"caption_ext\": \"txt\",\n",
            "            \"caption_dropout_rate\": 0.05,\n",
            "            \"shuffle_tokens\": false,\n",
            "            \"cache_latents_to_disk\": true,\n",
            "            \"resolution\": [\n",
            "                512,\n",
            "                768,\n",
            "                1024\n",
            "            ]\n",
            "        }\n",
            "    ],\n",
            "    \"train\": {\n",
            "        \"batch_size\": 1,\n",
            "        \"steps\": 2000,\n",
            "        \"gradient_accumulation_steps\": 1,\n",
            "        \"train_unet\": true,\n",
            "        \"train_text_encoder\": false,\n",
            "        \"gradient_checkpointing\": true,\n",
            "        \"noise_scheduler\": \"flowmatch\",\n",
            "        \"optimizer\": \"adamw8bit\",\n",
            "        \"lr\": 0.0001,\n",
            "        \"ema_config\": {\n",
            "            \"use_ema\": true,\n",
            "            \"ema_decay\": 0.99\n",
            "        },\n",
            "        \"dtype\": \"bf16\"\n",
            "    },\n",
            "    \"model\": {\n",
            "        \"name_or_path\": \"black-forest-labs/FLUX.1-dev\",\n",
            "        \"is_flux\": true,\n",
            "        \"quantize\": true\n",
            "    },\n",
            "    \"sample\": {\n",
            "        \"sampler\": \"flowmatch\",\n",
            "        \"sample_every\": 250,\n",
            "        \"width\": 1024,\n",
            "        \"height\": 1024,\n",
            "        \"prompts\": [\n",
            "            \"woman with red hair, playing chess at the park, bomb going off in the background\",\n",
            "            \"a woman holding a coffee cup, in a beanie, sitting at a cafe\",\n",
            "            \"a horse is a DJ at a night club, fish eye lens, smoke machine, lazer lights, holding a martini\",\n",
            "            \"a man showing off his cool new t shirt at the beach, a shark is jumping out of the water in the background\",\n",
            "            \"a bear building a log cabin in the snow covered mountains\",\n",
            "            \"woman playing the guitar, on stage, singing a song, laser lights, punk rocker\",\n",
            "            \"hipster man with a beard, building a chair, in a wood shop\",\n",
            "            \"photo of a man, white background, medium shot, modeling clothing, studio lighting, white backdrop\",\n",
            "            \"a man holding a sign that says, 'this is a sign'\",\n",
            "            \"a bulldog, in a post apocalyptic world, with a shotgun, in a leather jacket, in a desert, with a motorcycle\"\n",
            "        ],\n",
            "        \"neg\": \"\",\n",
            "        \"seed\": 42,\n",
            "        \"walk_seed\": true,\n",
            "        \"guidance_scale\": 4,\n",
            "        \"sample_steps\": 20\n",
            "    }\n",
            "}\n",
            "Using EMA\n",
            "/content/ai-toolkit/extensions_built_in/sd_trainer/SDTrainer.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n",
            "\n",
            "#############################################\n",
            "# Running job: my_first_flux_lora_v1\n",
            "#############################################\n",
            "\n",
            "\n",
            "Running  1 process\n",
            "Loading Flux model\n",
            "Loading transformer\n",
            "transformer/config.json: 100% 378/378 [00:00<00:00, 2.11MB/s]\n",
            "(…)ion_pytorch_model.safetensors.index.json: 100% 121k/121k [00:00<00:00, 944kB/s]\n",
            "(…)pytorch_model-00001-of-00003.safetensors: 100% 9.98G/9.98G [00:29<00:00, 344MB/s]\n",
            "(…)pytorch_model-00002-of-00003.safetensors: 100% 9.95G/9.95G [00:26<00:00, 373MB/s]\n",
            "(…)pytorch_model-00003-of-00003.safetensors: 100% 3.87G/3.87G [00:11<00:00, 338MB/s]\n",
            "Quantizing transformer\n",
            "scheduler/scheduler_config.json: 100% 274/274 [00:00<00:00, 2.10MB/s]\n",
            "Loading vae\n",
            "vae/config.json: 100% 774/774 [00:00<00:00, 6.35MB/s]\n",
            "diffusion_pytorch_model.safetensors: 100% 168M/168M [00:00<00:00, 202MB/s]  \n",
            "Loading t5\n",
            "tokenizer_2/tokenizer_config.json: 100% 20.8k/20.8k [00:00<00:00, 76.9MB/s]\n",
            "spiece.model: 100% 792k/792k [00:00<00:00, 19.4MB/s]\n",
            "tokenizer_2/tokenizer.json: 100% 2.42M/2.42M [00:00<00:00, 9.13MB/s]\n",
            "tokenizer_2/special_tokens_map.json: 100% 2.54k/2.54k [00:00<00:00, 18.6MB/s]\n",
            "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
            "text_encoder_2/config.json: 100% 782/782 [00:00<00:00, 6.13MB/s]\n",
            "(…)t_encoder_2/model.safetensors.index.json: 100% 19.9k/19.9k [00:00<00:00, 85.7MB/s]\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/4.99G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 10.5M/4.99G [00:00<04:24, 18.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 83.9M/4.99G [00:00<00:29, 164MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 220M/4.99G [00:00<00:11, 434MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 315M/4.99G [00:00<00:09, 489MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 398M/4.99G [00:01<00:11, 413MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 461M/4.99G [00:01<00:12, 368MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 514M/4.99G [00:01<00:13, 329MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 577M/4.99G [00:01<00:11, 382MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 629M/4.99G [00:01<00:12, 352MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 671M/4.99G [00:02<00:12, 339MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 765M/4.99G [00:02<00:09, 432MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 818M/4.99G [00:02<00:09, 439MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 870M/4.99G [00:02<00:09, 423MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 923M/4.99G [00:02<00:12, 324MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.03G/4.99G [00:02<00:08, 457MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.09G/4.99G [00:02<00:08, 460MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.14G/4.99G [00:03<00:09, 421MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.21G/4.99G [00:03<00:08, 452MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.30G/4.99G [00:03<00:06, 558MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.42G/4.99G [00:03<00:05, 697MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.52G/4.99G [00:03<00:04, 778MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.60G/4.99G [00:03<00:05, 668MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.68G/4.99G [00:03<00:06, 530MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.74G/4.99G [00:04<00:06, 515MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.80G/4.99G [00:04<00:06, 466MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.88G/4.99G [00:04<00:06, 502MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.94G/4.99G [00:04<00:07, 396MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.99G/4.99G [00:04<00:08, 360MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.03G/4.99G [00:04<00:08, 366MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.08G/4.99G [00:06<00:40, 71.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.20G/4.99G [00:07<00:20, 134MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.30G/4.99G [00:07<00:14, 190MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.41G/4.99G [00:07<00:09, 276MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.52G/4.99G [00:07<00:06, 361MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.69G/4.99G [00:07<00:04, 557MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.81G/4.99G [00:07<00:03, 635MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.92G/4.99G [00:07<00:02, 697MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.03G/4.99G [00:07<00:02, 781MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.14G/4.99G [00:08<00:02, 736MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.26G/4.99G [00:08<00:02, 834MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.37G/4.99G [00:08<00:02, 639MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.45G/4.99G [00:08<00:02, 587MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.52G/4.99G [00:08<00:02, 548MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.60G/4.99G [00:08<00:02, 532MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.66G/4.99G [00:11<00:12, 106MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.78G/4.99G [00:11<00:07, 163MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.90G/4.99G [00:11<00:04, 238MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 4.01G/4.99G [00:11<00:03, 310MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.10G/4.99G [00:11<00:02, 369MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.22G/4.99G [00:11<00:01, 475MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.35G/4.99G [00:11<00:01, 604MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.54G/4.99G [00:11<00:00, 829MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.81G/4.99G [00:11<00:00, 1.19GB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.99G/4.99G [00:12<00:00, 406MB/s] \n",
            "Downloading shards:  50% 1/2 [00:12<00:12, 12.42s/it]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/4.53G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   0% 10.5M/4.53G [00:00<03:47, 19.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 41.9M/4.53G [00:00<00:56, 79.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   3% 115M/4.53G [00:00<00:20, 218MB/s]  \u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 231M/4.53G [00:00<00:09, 435MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   8% 367M/4.53G [00:00<00:06, 644MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  10% 461M/4.53G [00:01<00:06, 640MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  12% 545M/4.53G [00:01<00:06, 572MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 619M/4.53G [00:01<00:07, 548MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 682M/4.53G [00:01<00:07, 538MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 744M/4.53G [00:01<00:07, 513MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 807M/4.53G [00:01<00:09, 400MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 891M/4.53G [00:02<00:07, 476MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 954M/4.53G [00:02<00:07, 463MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 1.01G/4.53G [00:02<00:08, 435MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 1.06G/4.53G [00:02<00:10, 317MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  25% 1.11G/4.53G [00:02<00:10, 312MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  25% 1.15G/4.53G [00:02<00:10, 315MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 1.20G/4.53G [00:03<00:12, 270MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 1.24G/4.53G [00:03<00:12, 272MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  28% 1.27G/4.53G [00:03<00:16, 199MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 1.30G/4.53G [00:03<00:15, 202MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 1.37G/4.53G [00:03<00:10, 292MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 1.42G/4.53G [00:04<00:09, 316MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 1.49G/4.53G [00:04<00:08, 379MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 1.54G/4.53G [00:04<00:08, 351MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 1.59G/4.53G [00:04<00:07, 369MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  36% 1.64G/4.53G [00:04<00:08, 354MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 1.68G/4.53G [00:06<00:45, 62.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  39% 1.78G/4.53G [00:06<00:24, 114MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.91G/4.53G [00:07<00:13, 195MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 2.11G/4.53G [00:07<00:06, 357MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 2.22G/4.53G [00:07<00:05, 444MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 2.34G/4.53G [00:07<00:04, 524MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  54% 2.45G/4.53G [00:07<00:03, 620MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 2.56G/4.53G [00:07<00:02, 683MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 2.67G/4.53G [00:07<00:02, 774MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 2.78G/4.53G [00:07<00:02, 722MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  63% 2.87G/4.53G [00:07<00:02, 748MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 2.97G/4.53G [00:08<00:02, 716MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 3.07G/4.53G [00:08<00:02, 696MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 3.16G/4.53G [00:08<00:02, 672MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 3.23G/4.53G [00:08<00:02, 521MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 3.29G/4.53G [00:08<00:02, 447MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  74% 3.35G/4.53G [00:10<00:12, 96.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 3.46G/4.53G [00:11<00:07, 152MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 3.53G/4.53G [00:11<00:05, 190MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 3.71G/4.53G [00:11<00:02, 334MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 3.86G/4.53G [00:11<00:01, 464MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 4.08G/4.53G [00:11<00:00, 703MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 4.30G/4.53G [00:11<00:00, 948MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 4.53G/4.53G [00:11<00:00, 383MB/s] \n",
            "Downloading shards: 100% 2/2 [00:24<00:00, 12.21s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:00<00:00,  3.10it/s]\n",
            "Quantizing T5\n",
            "Loading clip\n",
            "text_encoder/config.json: 100% 613/613 [00:00<00:00, 4.31MB/s]\n",
            "model.safetensors: 100% 246M/246M [00:01<00:00, 242MB/s] \n",
            "tokenizer/tokenizer_config.json: 100% 705/705 [00:00<00:00, 5.08MB/s]\n",
            "tokenizer/vocab.json: 100% 1.06M/1.06M [00:00<00:00, 5.42MB/s]\n",
            "tokenizer/merges.txt: 100% 525k/525k [00:00<00:00, 4.05MB/s]\n",
            "tokenizer/special_tokens_map.json: 100% 588/588 [00:00<00:00, 4.50MB/s]\n",
            "making pipe\n",
            "preparing\n",
            "create LoRA network. base dim (rank): 16, alpha: 16\n",
            "neuron dropout: p=None, rank dropout: p=None, module dropout: p=None\n",
            "create LoRA for Text Encoder: 0 modules.\n",
            "create LoRA for U-Net: 494 modules.\n",
            "enable LoRA for U-Net\n",
            "Dataset: /content/dataset\n",
            "  -  Preprocessing image dimensions\n",
            "100% 5/5 [00:00<00:00, 11735.60it/s]\n",
            "  -  Found 5 images\n",
            "Bucket sizes for /content/dataset:\n",
            "128x192: 3 files\n",
            "192x128: 2 files\n",
            "2 buckets made\n",
            "Caching latents for /content/dataset\n",
            " - Saving latents to disk\n",
            "Caching latents to disk: 100% 5/5 [00:01<00:00,  3.98it/s]\n",
            "Dataset: /content/dataset\n",
            "  -  Preprocessing image dimensions\n",
            "100% 5/5 [00:00<00:00, 25700.39it/s]\n",
            "  -  Found 5 images\n",
            "Bucket sizes for /content/dataset:\n",
            "128x192: 3 files\n",
            "192x128: 2 files\n",
            "2 buckets made\n",
            "Caching latents for /content/dataset\n",
            " - Saving latents to disk\n",
            "Caching latents to disk: 100% 5/5 [00:00<00:00, 14905.13it/s]\n",
            "Dataset: /content/dataset\n",
            "  -  Preprocessing image dimensions\n",
            "100% 5/5 [00:00<00:00, 26019.26it/s]\n",
            "  -  Found 5 images\n",
            "Bucket sizes for /content/dataset:\n",
            "128x192: 3 files\n",
            "192x128: 2 files\n",
            "2 buckets made\n",
            "Caching latents for /content/dataset\n",
            " - Saving latents to disk\n",
            "Caching latents to disk: 100% 5/5 [00:00<00:00, 15375.01it/s]\n",
            "Generating baseline samples before training\n",
            "my_first_flux_lora_v1:   0% 0/2000 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
            "my_first_flux_lora_v1:  12% 249/2000 [07:28<52:51,  1.81s/it, lr: 1.0e-04 loss: 1.878e-01]\n",
            "Generating Images:   0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "Generating Images:  10% 1/10 [00:18<02:44, 18.25s/it]\u001b[A\n",
            "Generating Images:  20% 2/10 [00:36<02:25, 18.20s/it]\u001b[A\n",
            "Generating Images:  30% 3/10 [00:54<02:07, 18.18s/it]\u001b[A\n",
            "Generating Images:  40% 4/10 [01:12<01:49, 18.17s/it]\u001b[A\n",
            "Generating Images:  50% 5/10 [01:30<01:30, 18.17s/it]\u001b[A\n",
            "Generating Images:  60% 6/10 [01:49<01:12, 18.17s/it]\u001b[A\n",
            "Generating Images:  70% 7/10 [02:07<00:54, 18.16s/it]\u001b[A\n",
            "Generating Images:  80% 8/10 [02:25<00:36, 18.16s/it]\u001b[A\n",
            "Generating Images:  90% 9/10 [02:43<00:18, 18.16s/it]\u001b[A\n",
            "Generating Images: 100% 10/10 [03:01<00:00, 18.16s/it]\u001b[A\n",
            "Saving at step 250\n",
            "Saved to output/my_first_flux_lora_v1/optimizer.pt\n",
            "my_first_flux_lora_v1:  25% 499/2000 [14:51<42:52,  1.71s/it, lr: 1.0e-04 loss: 6.945e-02]\n",
            "Generating Images:   0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "Generating Images:  10% 1/10 [00:18<02:44, 18.23s/it]\u001b[A\n",
            "Generating Images:  20% 2/10 [00:36<02:25, 18.19s/it]\u001b[A"
          ]
        }
      ],
      "source": [
        "!cd ai-toolkit && python run.py \"config/examples/train_lora_flux_24gb.yaml\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AkUxPzdy88b"
      },
      "source": [
        "#### Part 7: Verify that the Model has Trained Correctly.\n",
        "\n",
        "To verify that your model has trained correctly follow the steps below:\n",
        "\n",
        "1) Inside the `ai-toolkit` folder, locate the `output` folder and navigate inside the `my_first_flux_lora_v1/samples` folder.\n",
        "\n",
        "2) Compare the output images at different timesteps and observe how the original AI images change based on the database images you provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukW_ymWu0R48"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
